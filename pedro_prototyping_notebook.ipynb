{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iefbI3c9YWRf"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%%shell\n",
        "git clone https://github.com/plarotta/vision-encoder.git\n",
        "mv /content/vision-encoder/transfer_learner/ /content/\n",
        "rm -rf /content/vision-encoder/\n",
        "# wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1zxFDf6wqacM4EfMzLp2YD7kI9g-3KtJk' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1zxFDf6wqacM4EfMzLp2YD7kI9g-3KtJk\" -O UR5_images_2.zip && rm -rf /tmp/cookies.txt\n",
        "# wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1TAMiAfoJIyFmvGo5gLzxftRHprF60PzG' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1TAMiAfoJIyFmvGo5gLzxftRHprF60PzG\" -O UR5_positions_2.csv && rm -rf /tmp/cookies.txt\n",
        "wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1M16MVlj7pFdaw_JB0iwEe2xFlmeBhgkY' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1M16MVlj7pFdaw_JB0iwEe2xFlmeBhgkY\" -O UR5_images_3.zip && rm -rf /tmp/cookies.txt\n",
        "wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1CDgE1eaYw0R-PdWPWFRXKWpowmSN7pMh' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1CDgE1eaYw0R-PdWPWFRXKWpowmSN7pMh\" -O UR5_positions_3.csv && rm -rf /tmp/cookies.txt\n",
        "unzip UR5_images_3.zip\n",
        "mv UR5_images_3/ images/\n",
        "rm UR5_images_3.zip\n",
        "pip install wandb -qq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XP2-2yGSZHV0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bc15262-ef00-4d84-c9ac-5a4de116fb3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
            "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
            "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torch\n",
        "import os\n",
        "from skimage import io\n",
        "from torchvision.transforms import v2\n",
        "from matplotlib import pyplot as plt\n",
        "from torchvision.models.resnet import ResNet18_Weights\n",
        "from torchvision.models import resnet18\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from transfer_learner.source.model import RobotNet\n",
        "from transfer_learner.source.robot_dataset import RobotImageDataset\n",
        "from transfer_learner.utils.trainers import train_one_epoch, validate_one_epoch\n",
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lR4PTBIggEFb"
      },
      "outputs": [],
      "source": [
        "class RobotImageDataset(Dataset):\n",
        "    #TODO: make multi_input = False functional. Right now it fails\n",
        "    def __init__(self, csv_file: str, root_dir: str, multi_input = False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            csv_file (string): Path to the csv file with annotations.\n",
        "            root_dir (string): Directory with all the images.\n",
        "        \"\"\"\n",
        "        self.annotations = pd.read_csv(csv_file)\n",
        "        self.annotations = self.annotations[['ImageID','Joint3','Joint4','Joint5', 'Joint6']]\n",
        "        self.root_dir = root_dir\n",
        "        self.multi_input = multi_input\n",
        "        self.file_names = [\"side_view_\",\n",
        "                           \"front_view_\",\n",
        "                           \"top_view_\",\n",
        "                           \"corner1_view_\",\n",
        "                           \"corner2_view_\",\n",
        "                           \"side_depth_view_\",\n",
        "                           \"front_depth_view_\",\n",
        "                           \"top_depth_view_\",\n",
        "                           \"corner1_depth_view_\",\n",
        "                           \"corner2_depth_view_\"]\n",
        "\n",
        "        #TODO: improve transform so that the final image view is more zoomed in\n",
        "\n",
        "        if self.multi_input:\n",
        "            self.transform = v2.Compose(\n",
        "                [\n",
        "                    v2.ToDtype(torch.float32),\n",
        "                    v2.ToTensor(),\n",
        "                    v2.Resize(size=(224, 224), antialias=True),\n",
        "                    v2.Normalize(mean=[0.485, 0.456, 0.406], #default resnet norm\n",
        "                                std=[0.229, 0.224, 0.225]),\n",
        "                    v2.ToTensor()\n",
        "                ])\n",
        "        else:\n",
        "            self.transform = v2.Compose(\n",
        "                [\n",
        "                    v2.ToDtype(torch.float32),\n",
        "                    v2.ToTensor(),\n",
        "                    v2.Resize(size=(224, 224), antialias=True),\n",
        "                    v2.Grayscale(),\n",
        "                    v2.ToTensor()\n",
        "                ])\n",
        "\n",
        "    def process_images(self, images):\n",
        "        for im_idx in range(len(images)):\n",
        "            if len(images[im_idx].shape) > 2:\n",
        "                images[im_idx] = images[im_idx][:,:,:3]\n",
        "            images[im_idx] = self.transform(images[im_idx])\n",
        "        out = [im for im in images] if self.multi_input else torch.cat([im for im in images],dim = 0)\n",
        "        return(out)\n",
        "\n",
        "    def __len__(self):\n",
        "        return(len(self.annotations))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "        images = [io.imread(os.path.join(self.root_dir, f + str(idx) + '.png')) for f in self.file_names]\n",
        "        im_data = self.process_images(images)\n",
        "        joint_values = self.annotations.iloc[idx, 1:].to_numpy(dtype=float)\n",
        "        sample = {'images': im_data, 'joint_values': joint_values}\n",
        "        return sample"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "robotdata = RobotImageDataset('/content/UR5_positions_3.csv', '/content/images', multi_input=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86kvEnxsfHmP",
        "outputId": "1276c0a8-e65f-4aad-b945-bd4244aa3f13"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/v2/_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `transforms.Compose([transforms.ToImageTensor(), transforms.ConvertImageDtype()])`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(robotdata[0]['images'].shape)\n",
        "plt.imshow(robotdata[0]['images'][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "3DZ0ptxefKoc",
        "outputId": "26a9ab8f-2217-49d5-8e03-f0ae10051f71"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 224, 224])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7d16ec664670>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XRzxvs2C4hJL"
      },
      "outputs": [],
      "source": [
        "# RUN THIS MODEL!!!\n",
        "class RobotNet(nn.Module):\n",
        "    def __init__(self, n_hidden_units1, n_hidden_units2, n_inputs=1, freeze_resnet = False):\n",
        "        super().__init__()\n",
        "        self.num_inputs = n_inputs\n",
        "        self.in_dim1 = n_hidden_units1\n",
        "        self.in_dim2 = n_hidden_units2\n",
        "        self.weights = ResNet18_Weights.DEFAULT\n",
        "\n",
        "        self.base_model = resnet18(weights=self.weights)\n",
        "        self.base_layers = list(self.base_model.children())\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        # freeze resnet layers\n",
        "        if freeze_resnet:\n",
        "            for idx,child in enumerate(self.base_layers):\n",
        "              for param in child.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "\n",
        "        self.resnet_layers = nn.Sequential(\n",
        "            nn.Conv2d(10, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False),\n",
        "            *self.base_layers[1:-1])\n",
        "\n",
        "        self.linear_output = nn.Sequential(\n",
        "            nn.Linear(in_features=512*self.num_inputs, out_features=self.in_dim1),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=self.in_dim1, out_features=self.in_dim2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=self.in_dim2, out_features=4)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, inputs):\n",
        "\n",
        "        if self.num_inputs > 1:\n",
        "            for idx in range(len(inputs)):\n",
        "                inputs[idx] = self.resnet_layers(inputs[idx])\n",
        "            out = self.linear_output(torch.cat(inputs,dim=1).view(-1,512*len(inputs)))\n",
        "        else:\n",
        "            out = self.resnet_layers(inputs)\n",
        "            out = self.linear_output(self.flatten(out))\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "d2ba38e3ec5c4439884750c582ee07da",
            "4407f08d0a4b4b9abe24c44249111bb3",
            "c3969c4bcfec4d658f1566a2c73c55cd",
            "12059b2cbe8440f4a2be1c91fbd1ed86",
            "4a727dde428f47abb0856774ca932083",
            "01da5be5c1924e5a88dded1a57f2b955",
            "9797e9c84297407c83dae6eaa98ae010",
            "6fa03d5c10a1450b93a93ed636fdee27"
          ]
        },
        "id": "7bcCLJ-zaKgx",
        "outputId": "fa79a20a-67b1-4f82-c4f2-1a29877edcd0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/v2/_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `transforms.Compose([transforms.ToImageTensor(), transforms.ConvertImageDtype()])`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.16.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20231204_022431-yp5k5i97</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/personal646/vision-encoder/runs/yp5k5i97' target=\"_blank\">mild-rain-41</a></strong> to <a href='https://wandb.ai/personal646/vision-encoder' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/personal646/vision-encoder' target=\"_blank\">https://wandb.ai/personal646/vision-encoder</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/personal646/vision-encoder/runs/yp5k5i97' target=\"_blank\">https://wandb.ai/personal646/vision-encoder/runs/yp5k5i97</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 65.4MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1 | train loss: 0.080116210475564 | val loss: 0.03143449322808357 | \"train_feedback\": 0.17515040537714957 | \"val_feedback\": 0.06533923745155334\n",
            "epoch: 2 | train loss: 0.02382286062464118 | val loss: 0.01975704330418791 | \"train_feedback\": 0.048949614115059374 | \"val_feedback\": 0.040636900812387466\n",
            "epoch: 3 | train loss: 0.016372106993570924 | val loss: 0.014759953843340987 | \"train_feedback\": 0.03325502410903573 | \"val_feedback\": 0.03000614605844021\n",
            "epoch: 4 | train loss: 0.012648445716127754 | val loss: 0.011595853064800538 | \"train_feedback\": 0.02557595120370388 | \"val_feedback\": 0.023546267300844193\n",
            "epoch: 5 | train loss: 0.010196672932244837 | val loss: 0.010518932035045019 | \"train_feedback\": 0.02056405888311565 | \"val_feedback\": 0.021355168893933296\n",
            "epoch: 6 | train loss: 0.009289134982973337 | val loss: 0.00938397589775305 | \"train_feedback\": 0.018723077110946177 | \"val_feedback\": 0.01902698166668415\n",
            "epoch: 7 | train loss: 0.0076891130106523636 | val loss: 0.008236638399668865 | \"train_feedback\": 0.015479767965152859 | \"val_feedback\": 0.016661524772644043\n",
            "epoch: 8 | train loss: 0.006423398959450424 | val loss: 0.006596065052444973 | \"train_feedback\": 0.012911587269976736 | \"val_feedback\": 0.013303501531481743\n",
            "epoch: 9 | train loss: 0.005910681156441569 | val loss: 0.008224027615690988 | \"train_feedback\": 0.011959443265572191 | \"val_feedback\": 0.01659965328872204\n",
            "epoch: 10 | train loss: 0.006054644751362502 | val loss: 0.006002248782250617 | \"train_feedback\": 0.012178452571853996 | \"val_feedback\": 0.012087926268577576\n",
            "epoch: 11 | train loss: 0.004956099811941385 | val loss: 0.00509772284902514 | \"train_feedback\": 0.009949667965993286 | \"val_feedback\": 0.01025755237787962\n",
            "epoch: 12 | train loss: 0.003991823226679117 | val loss: 0.004664054776113185 | \"train_feedback\": 0.008008012256585062 | \"val_feedback\": 0.009383471682667732\n",
            "epoch: 13 | train loss: 0.0057119704633951185 | val loss: 0.006501100244118817 | \"train_feedback\": 0.01169131103157997 | \"val_feedback\": 0.013195415958762169\n",
            "epoch: 14 | train loss: 0.0057915555126965045 | val loss: 0.005056806318905382 | \"train_feedback\": 0.011863264504820108 | \"val_feedback\": 0.010314102284610271\n",
            "epoch: 15 | train loss: 0.003475019067991525 | val loss: 0.004214851130243568 | \"train_feedback\": 0.00696850454993546 | \"val_feedback\": 0.00850757583975792\n",
            "epoch: 16 | train loss: 0.003202288262080401 | val loss: 0.004056329073916588 | \"train_feedback\": 0.006420181667432189 | \"val_feedback\": 0.00821216031908989\n",
            "epoch: 17 | train loss: 0.003690510249696672 | val loss: 0.004046224063217994 | \"train_feedback\": 0.007410494716838002 | \"val_feedback\": 0.00816335342824459\n",
            "epoch: 18 | train loss: 0.0030841819304041566 | val loss: 0.0038843570861019313 | \"train_feedback\": 0.006187189957126975 | \"val_feedback\": 0.007853594608604908\n",
            "epoch: 19 | train loss: 0.002942100987304002 | val loss: 0.0031792151978209853 | \"train_feedback\": 0.0058975501963868734 | \"val_feedback\": 0.00638595363125205\n",
            "epoch: 20 | train loss: 0.0026709187147207557 | val loss: 0.0032033660460174793 | \"train_feedback\": 0.005353170349262654 | \"val_feedback\": 0.006483536679297686\n",
            "epoch: 21 | train loss: 0.0024225069698877635 | val loss: 0.0038517528021382906 | \"train_feedback\": 0.004855161442421377 | \"val_feedback\": 0.007766698952764273\n",
            "epoch: 22 | train loss: 0.0023469099504873155 | val loss: 0.003167500943019395 | \"train_feedback\": 0.004702110195532441 | \"val_feedback\": 0.006382739637047052\n",
            "epoch: 23 | train loss: 0.008472954725846648 | val loss: 0.006673362239130906 | \"train_feedback\": 0.017888846528716387 | \"val_feedback\": 0.013676767237484455\n",
            "epoch: 24 | train loss: 0.00339493593852967 | val loss: 0.003540617801869909 | \"train_feedback\": 0.0068276154836639765 | \"val_feedback\": 0.007120745722204447\n",
            "epoch: 25 | train loss: 0.0030678891856223346 | val loss: 0.0027570557274249575 | \"train_feedback\": 0.006403859417885542 | \"val_feedback\": 0.005540912970900536\n",
            "epoch: 26 | train loss: 0.0024198185661807657 | val loss: 0.002631032769198692 | \"train_feedback\": 0.0048832843014970425 | \"val_feedback\": 0.005285446997731924\n",
            "epoch: 27 | train loss: 0.001961079647298902 | val loss: 0.0025326296232552046 | \"train_feedback\": 0.003928640522062778 | \"val_feedback\": 0.005083980038762093\n",
            "epoch: 28 | train loss: 0.0016442162187304348 | val loss: 0.002326171701064422 | \"train_feedback\": 0.003292454522103071 | \"val_feedback\": 0.004674658644944429\n",
            "epoch: 29 | train loss: 0.0015488282043952496 | val loss: 0.0021439704449019497 | \"train_feedback\": 0.0031012900942005216 | \"val_feedback\": 0.0043082404881715775\n",
            "epoch: 30 | train loss: 0.001608914251672104 | val loss: 0.0024517569155062713 | \"train_feedback\": 0.0032220346820540724 | \"val_feedback\": 0.004922810941934586\n",
            "epoch: 31 | train loss: 0.0014839249381329864 | val loss: 0.0021286315159014767 | \"train_feedback\": 0.0029712309609167278 | \"val_feedback\": 0.004274980630725622\n",
            "epoch: 32 | train loss: 0.0015930952953640371 | val loss: 0.002666756680737885 | \"train_feedback\": 0.0031900913282297554 | \"val_feedback\": 0.0053579495288431644\n",
            "epoch: 33 | train loss: 0.0024583775550127028 | val loss: 0.0027229588733808626 | \"train_feedback\": 0.004983273633755744 | \"val_feedback\": 0.005464726127684116\n",
            "epoch: 34 | train loss: 0.0017660062601789833 | val loss: 0.002265289144974852 | \"train_feedback\": 0.0035368424956686794 | \"val_feedback\": 0.004545480478554964\n",
            "epoch: 35 | train loss: 0.0015087980753742158 | val loss: 0.0023536336301485934 | \"train_feedback\": 0.0030210610381327567 | \"val_feedback\": 0.0047217165119946\n",
            "epoch: 36 | train loss: 0.0014343471557367593 | val loss: 0.0020575610244469274 | \"train_feedback\": 0.0028718575607053934 | \"val_feedback\": 0.004131773952394724\n",
            "epoch: 37 | train loss: 0.0015792447437997907 | val loss: 0.0023598836601844858 | \"train_feedback\": 0.003162399326916784 | \"val_feedback\": 0.0047381725162267685\n",
            "epoch: 38 | train loss: 0.0014118200866505503 | val loss: 0.0019276616097028767 | \"train_feedback\": 0.002826724340207875 | \"val_feedback\": 0.0038764069322496653\n",
            "epoch: 39 | train loss: 0.001516917216591537 | val loss: 0.002021427002055423 | \"train_feedback\": 0.0030376213202252984 | \"val_feedback\": 0.004058964550495148\n",
            "epoch: 40 | train loss: 0.006309373018797487 | val loss: 0.015320629169720979 | \"train_feedback\": 0.013586853905115277 | \"val_feedback\": 0.03351976349949837\n",
            "epoch: 41 | train loss: 0.004808158142026514 | val loss: 0.004423197458631226 | \"train_feedback\": 0.009839051582850516 | \"val_feedback\": 0.008918285369873047\n",
            "epoch: 42 | train loss: 0.001807442392455414 | val loss: 0.002143407757911417 | \"train_feedback\": 0.003621804879978299 | \"val_feedback\": 0.004306250251829624\n",
            "epoch: 43 | train loss: 0.0015540149197913707 | val loss: 0.001837408752180636 | \"train_feedback\": 0.003164767842274159 | \"val_feedback\": 0.0036872243508696556\n",
            "epoch: 44 | train loss: 0.001037300100317225 | val loss: 0.0015140463125019792 | \"train_feedback\": 0.0020762591799721123 | \"val_feedback\": 0.0030350012239068747\n",
            "epoch: 45 | train loss: 0.0010109379452187567 | val loss: 0.0016815998269954608 | \"train_feedback\": 0.002023470837622881 | \"val_feedback\": 0.0033724803943187\n",
            "epoch: 46 | train loss: 0.0009062524263281375 | val loss: 0.00185541640264943 | \"train_feedback\": 0.0018136828150600195 | \"val_feedback\": 0.0037231130991131067\n",
            "epoch: 47 | train loss: 0.0009262054945575073 | val loss: 0.001612639248681565 | \"train_feedback\": 0.001853633509017527 | \"val_feedback\": 0.003254153300076723\n",
            "epoch: 48 | train loss: 0.0009325058194808662 | val loss: 0.001845283042048178 | \"train_feedback\": 0.0018663968294858932 | \"val_feedback\": 0.003706812160089612\n",
            "epoch: 49 | train loss: 0.0009315631568897516 | val loss: 0.001724193035255349 | \"train_feedback\": 0.0018644576692022382 | \"val_feedback\": 0.0034579900093376637\n",
            "epoch: 50 | train loss: 0.0009844034521374852 | val loss: 0.001797989790608722 | \"train_feedback\": 0.0019702082858420907 | \"val_feedback\": 0.00361145893111825\n",
            "epoch: 51 | train loss: 0.0009237827959004789 | val loss: 0.0014486283429765276 | \"train_feedback\": 0.0018487851561512798 | \"val_feedback\": 0.0029048658907413483\n",
            "epoch: 52 | train loss: 0.0010138602708466352 | val loss: 0.0018852890860880651 | \"train_feedback\": 0.0020294271702878176 | \"val_feedback\": 0.003782418556511402\n",
            "epoch: 53 | train loss: 0.0009722026011440903 | val loss: 0.0016864699307858708 | \"train_feedback\": 0.0019458596864715219 | \"val_feedback\": 0.0033818846568465233\n",
            "epoch: 54 | train loss: 0.0009811980447266251 | val loss: 0.0017598381336216653 | \"train_feedback\": 0.0019640294327400625 | \"val_feedback\": 0.003531467402353883\n",
            "epoch: 55 | train loss: 0.0010103958053514362 | val loss: 0.0017442219328903962 | \"train_feedback\": 0.0020224321437999604 | \"val_feedback\": 0.003498953068628907\n",
            "epoch: 56 | train loss: 0.0012240102181676775 | val loss: 0.0017269563617273455 | \"train_feedback\": 0.0024506162302568553 | \"val_feedback\": 0.0034632771275937557\n",
            "epoch: 57 | train loss: 0.0015204622829332948 | val loss: 0.002001021611726 | \"train_feedback\": 0.003048045811243355 | \"val_feedback\": 0.004017837345600128\n",
            "epoch: 58 | train loss: 0.0011776343227829784 | val loss: 0.0018504679864568133 | \"train_feedback\": 0.0023578125671483577 | \"val_feedback\": 0.0037217249628156424\n",
            "epoch: 59 | train loss: 0.0010328344500157981 | val loss: 0.0017157934277155806 | \"train_feedback\": 0.00206747060501948 | \"val_feedback\": 0.003457678249105811\n",
            "epoch: 60 | train loss: 0.0009517060671932995 | val loss: 0.0017954373920691154 | \"train_feedback\": 0.0019049060656689107 | \"val_feedback\": 0.003605723148211837\n",
            "epoch: 61 | train loss: 0.0010640254211612046 | val loss: 0.0016143577624230631 | \"train_feedback\": 0.0021299406308680773 | \"val_feedback\": 0.003238801844418049\n",
            "epoch: 62 | train loss: 0.0009734201582614333 | val loss: 0.001557713594391114 | \"train_feedback\": 0.0019482991844415665 | \"val_feedback\": 0.0031251877080649137\n",
            "epoch: 63 | train loss: 0.0009263359707547352 | val loss: 0.0013908952694489723 | \"train_feedback\": 0.0018540737701114268 | \"val_feedback\": 0.002789250807836652\n",
            "epoch: 64 | train loss: 0.0009152575782500207 | val loss: 0.0014631108053782512 | \"train_feedback\": 0.001831915771588683 | \"val_feedback\": 0.0029451753944158554\n",
            "epoch: 65 | train loss: 0.000957895333878696 | val loss: 0.0016475553635419125 | \"train_feedback\": 0.00191726448899135 | \"val_feedback\": 0.0033083679154515266\n",
            "epoch: 66 | train loss: 0.0009009676231071352 | val loss: 0.0014489963392002715 | \"train_feedback\": 0.0018032171069644392 | \"val_feedback\": 0.0029059192165732384\n",
            "epoch: 67 | train loss: 0.0008861222679261118 | val loss: 0.0013342306880827343 | \"train_feedback\": 0.0017734727582428605 | \"val_feedback\": 0.002677049720659852\n",
            "epoch: 68 | train loss: 0.0008351876516826451 | val loss: 0.001562206646508818 | \"train_feedback\": 0.0016714960620738567 | \"val_feedback\": 0.003136279294267297\n",
            "epoch: 69 | train loss: 0.0008171246690908447 | val loss: 0.0013128063273763963 | \"train_feedback\": 0.0016352793825790287 | \"val_feedback\": 0.002632569521665573\n",
            "epoch: 70 | train loss: 0.0007904388793976978 | val loss: 0.0014892704406058387 | \"train_feedback\": 0.0015819846738595515 | \"val_feedback\": 0.0029897610656917095\n",
            "epoch: 71 | train loss: 0.00078253648604732 | val loss: 0.0013845654866022487 | \"train_feedback\": 0.0015659967258106918 | \"val_feedback\": 0.0027802856639027596\n",
            "epoch: 72 | train loss: 0.0008250127021456138 | val loss: 0.0017420116660466034 | \"train_feedback\": 0.0016511896254960447 | \"val_feedback\": 0.0034944673534482718\n",
            "epoch: 73 | train loss: 0.0009106566598638893 | val loss: 0.0017383671753729384 | \"train_feedback\": 0.0018226657025516033 | \"val_feedback\": 0.0034839515574276447\n",
            "epoch: 74 | train loss: 0.0008571750060655176 | val loss: 0.003089236904911342 | \"train_feedback\": 0.001717713940422982 | \"val_feedback\": 0.007034990005195141\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.003 MB of 0.003 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d2ba38e3ec5c4439884750c582ee07da"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_feedback</td><td>█▃▂▂▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▃▂▂▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_feedback</td><td>█▅▃▃▂▂▂▂▂▂▁▂▂▁▁▁▁▁▁▁▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▅▃▃▂▂▂▂▂▂▁▂▂▁▁▁▁▁▁▁▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>74</td></tr><tr><td>train_feedback</td><td>0.00172</td></tr><tr><td>train_loss</td><td>0.00086</td></tr><tr><td>val_feedback</td><td>0.00703</td></tr><tr><td>val_loss</td><td>0.00309</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">mild-rain-41</strong> at: <a href='https://wandb.ai/personal646/vision-encoder/runs/yp5k5i97' target=\"_blank\">https://wandb.ai/personal646/vision-encoder/runs/yp5k5i97</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 74 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20231204_022431-yp5k5i97/logs</code>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "robotdata = RobotImageDataset('/content/UR5_positions_3.csv', '/content/images', multi_input=False)\n",
        "\n",
        "\n",
        "def train_one_epoch(tr_loader, model, loss_func, optimizer, device,feedback=None):\n",
        "    running_loss = 0.0\n",
        "    fback = 0.0\n",
        "\n",
        "    for i, data in enumerate(tr_loader):\n",
        "        im1 = data['images']\n",
        "        y = data['joint_values'].float()\n",
        "        im1,y = im1.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        # print('model in',im1.shape)\n",
        "        preds = model(im1).float()\n",
        "        preds_hat = torch.cat([torch.sin(preds),torch.cos(preds)],dim=1)\n",
        "        y_hat = torch.cat([torch.sin(y),torch.cos(y)],dim=1)\n",
        "        loss = loss_func(preds_hat, y_hat)\n",
        "\n",
        "        running_loss +=loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if feedback:\n",
        "            fback += feedback(preds,y).item()\n",
        "\n",
        "\n",
        "    train_loss = running_loss/(i+1)\n",
        "    if feedback:\n",
        "        return(train_loss,fback/(i+1))\n",
        "    else:\n",
        "      return(train_loss)\n",
        "\n",
        "def validate_one_epoch(val_loader, model, loss_func, optimizer, device,feedback=None):\n",
        "    running_loss = 0.0\n",
        "    fback = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, data in enumerate(val_loader):\n",
        "            im1 = data['images']\n",
        "            y = data['joint_values'].float()\n",
        "            im1, y = im1.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(im1).float()\n",
        "            preds_hat = torch.cat([torch.sin(preds),torch.cos(preds)],dim=1)\n",
        "            y_hat = torch.cat([torch.sin(y),torch.cos(y)],dim=1)\n",
        "            loss = loss_func(preds_hat, y_hat)\n",
        "            running_loss +=loss.item()\n",
        "            if feedback:\n",
        "                fback += feedback(preds,y)\n",
        "    val_loss = running_loss/(i+1)\n",
        "\n",
        "    if feedback:\n",
        "        return(val_loss,fback/(i+1))\n",
        "    else:\n",
        "      return(val_loss)\n",
        "\n",
        "def main():\n",
        "    run = wandb.init(project='vision-encoder')\n",
        "    LEARNING_RATE = 2e-4\n",
        "    BATCH_SIZE = 32\n",
        "    EPOCH = 75\n",
        "    IN_DIM1 = 128\n",
        "    IN_DIM2 = 32\n",
        "\n",
        "\n",
        "    # Check if GPU is available\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(device)\n",
        "    # Params\n",
        "    traindata, validdata = random_split(robotdata, [0.8,0.2])\n",
        "    trainloader = DataLoader(traindata, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)\n",
        "    validloader = DataLoader(validdata, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)\n",
        "    multi_model = RobotNet(IN_DIM1, IN_DIM2,n_inputs=1)\n",
        "\n",
        "\n",
        "    # Initialize\n",
        "    multi_model.to(device)\n",
        "    criterion = nn.MSELoss()\n",
        "    feedback = nn.MSELoss()\n",
        "    optimizer = optim.AdamW(multi_model.parameters(), lr=LEARNING_RATE, weight_decay=0.0001)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max= 1000)\n",
        "\n",
        "    for epoch in range(1,EPOCH):\n",
        "\n",
        "        tr_loss,t_fback = train_one_epoch(trainloader, multi_model, criterion, optimizer, device, feedback)\n",
        "        val_loss,v_fback = validate_one_epoch(validloader, multi_model, criterion, optimizer, device, feedback)\n",
        "        wandb.log(\n",
        "            {\n",
        "                \"epoch\": epoch,\n",
        "                \"train_loss\": tr_loss,\n",
        "                \"val_loss\": val_loss,\n",
        "                \"train_feedback\": t_fback,\n",
        "                \"val_feedback\": v_fback\n",
        "            }\n",
        "        )\n",
        "        print(f'epoch: {epoch} | train loss: {tr_loss} | val loss: {val_loss} | \"train_feedback\": {t_fback} | \"val_feedback\": {v_fback}')\n",
        "        scheduler.step()\n",
        "        # saving to the wandb dir ensures that the model files are accessible on the portal once the run finishes\n",
        "        torch.save(multi_model.state_dict(),os.path.join(wandb.run.dir, f\"model_state_dict_epoch{epoch}.pth\"))\n",
        "    run.finish()\n",
        "    torch.save(multi_model.state_dict(), 'model_state.pth')\n",
        "\n",
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "multi_model = RobotNet(128,32)\n",
        "multi_model.load_state_dict(torch.load('/content/wandb/latest-run/files/model_state_dict_epoch70.pth'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwf7j1tUX-mo",
        "outputId": "0d69054b-a5d8-4198-94fe-8f1369c67dc4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "robotdata = RobotImageDataset('/content/UR5_positions_3.csv', '/content/images', multi_input=False)\n",
        "loader = DataLoader(robotdata, batch_size=10, shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRf00I3nYQx0",
        "outputId": "0203c8a0-b4c2-423b-feaa-8b43f7c4c8a3"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/v2/_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `transforms.Compose([transforms.ToImageTensor(), transforms.ConvertImageDtype()])`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "running_loss = 0.0\n",
        "fback = 0.0\n",
        "device = 'cpu'\n",
        "loss_f = nn.MSELoss()\n",
        "feed = nn.MSELoss()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i, data in enumerate(loader):\n",
        "        im1 = data['images']\n",
        "        y = data['joint_values'].float()\n",
        "        preds = multi_model(im1).float()\n",
        "        preds_hat = torch.cat([torch.sin(preds),torch.cos(preds)],dim=1)\n",
        "        y_hat = torch.cat([torch.sin(y),torch.cos(y)],dim=1)\n",
        "        loss = loss_f(preds_hat, y_hat)\n",
        "        running_loss +=loss.item()\n",
        "        feed_l = feed(preds,y)\n",
        "        fback += feed_l.item()\n",
        "        print(loss.item(),feed_l.item())\n",
        "        print(f'gt: {y}, pred: {preds}')\n",
        "        input()\n",
        "\n",
        "val_loss = running_loss/(i+1)\n",
        "feed_loss = fback/(i+1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 738
        },
        "id": "pxZTTAuzYTpY",
        "outputId": "687f0745-4f6d-4bd4-85c9-cdc0455a29ef"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0014974653022363782 0.0029966768343001604\n",
            "gt: tensor([[-0.9371, -1.5443,  0.1657,  0.1101],\n",
            "        [-1.5646, -0.0287, -0.7468, -1.2957],\n",
            "        [ 0.5278, -1.3250,  0.2066,  0.0881],\n",
            "        [ 0.6543,  0.5727, -0.5525,  0.7712],\n",
            "        [-0.1550,  0.0242,  1.2605,  0.0384],\n",
            "        [-0.2924,  0.0795, -1.2657, -0.6176],\n",
            "        [ 0.2246, -0.4158,  0.0580,  0.3474],\n",
            "        [ 0.4635, -0.4300,  1.3248, -0.8876],\n",
            "        [-0.2401, -1.1433,  0.6444,  1.0012],\n",
            "        [ 0.4013, -1.4451, -0.2776,  1.5260]]), pred: tensor([[-0.9624, -1.4749,  0.1441,  0.1512],\n",
            "        [-1.5664,  0.0124, -0.7704, -1.2280],\n",
            "        [ 0.5720, -1.2998,  0.2296,  0.0403],\n",
            "        [ 0.7130,  0.6698, -0.6481,  0.8246],\n",
            "        [-0.2014,  0.0744,  1.2545,  0.1440],\n",
            "        [-0.2896,  0.1460, -1.3044, -0.7105],\n",
            "        [ 0.2597, -0.3658,  0.0375,  0.2624],\n",
            "        [ 0.5394, -0.4446,  1.3474, -0.8950],\n",
            "        [-0.2788, -1.0843,  0.6643,  0.9910],\n",
            "        [ 0.4807, -1.3254, -0.2823,  1.4999]])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-2a56240a0d63>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_l\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'gt: {y}, pred: {preds}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunning_loss\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d2ba38e3ec5c4439884750c582ee07da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4407f08d0a4b4b9abe24c44249111bb3",
              "IPY_MODEL_c3969c4bcfec4d658f1566a2c73c55cd"
            ],
            "layout": "IPY_MODEL_12059b2cbe8440f4a2be1c91fbd1ed86"
          }
        },
        "4407f08d0a4b4b9abe24c44249111bb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a727dde428f47abb0856774ca932083",
            "placeholder": "​",
            "style": "IPY_MODEL_01da5be5c1924e5a88dded1a57f2b955",
            "value": "3335.666 MB of 3335.666 MB uploaded\r"
          }
        },
        "c3969c4bcfec4d658f1566a2c73c55cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9797e9c84297407c83dae6eaa98ae010",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6fa03d5c10a1450b93a93ed636fdee27",
            "value": 1
          }
        },
        "12059b2cbe8440f4a2be1c91fbd1ed86": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a727dde428f47abb0856774ca932083": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "01da5be5c1924e5a88dded1a57f2b955": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9797e9c84297407c83dae6eaa98ae010": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6fa03d5c10a1450b93a93ed636fdee27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}